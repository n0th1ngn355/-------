{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d796cef23297fd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T15:04:14.363498Z",
     "start_time": "2024-03-26T15:03:34.764735Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 1.65k/1.65k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|██████████| 276M/276M [01:28<00:00, 3.11MB/s] \n",
      "Downloading data: 100%|██████████| 32.0M/32.0M [00:06<00:00, 5.28MB/s]\n",
      "Generating train split: 189891 examples [00:04, 43466.09 examples/s]\n",
      "Generating validation split: 21098 examples [00:00, 42843.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from heapq import heappush, heappop\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"MonoHime/ru_sentiment_dataset\")\n",
    "sentences = dataset['train']['text']\n",
    "text = \"\"\n",
    "for i in range(10000, len(sentences) // 10):\n",
    "    text += sentences[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7331b8cca5d95e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T15:07:08.178444Z",
     "start_time": "2024-03-26T15:04:14.388491Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' абвгдежзийклмнопрстуфхцчшщъыьэюя'\n",
      "0 -15248.792990777416\n",
      "1 -12872.423467606657\n",
      "2 -10734.061519962917\n",
      "3 -8668.154229419846\n",
      "4 -7166.513216102878\n"
     ]
    }
   ],
   "source": [
    "class LanguageNgramModel:\n",
    "    \"\"\" Модель запоминает и предсказывает, за какими буквами следуют какие.\n",
    "    Параметры конструктора:\n",
    "    order - порядок (сколько предыдущих букв помнит модель), или n-1\n",
    "    smoothing - величина, добавляемая к каждому счётчику букв для устойчивости\n",
    "    recursive - вес, с которым используется модель на один порядок меньше\n",
    "    Обучаемые параметры:\n",
    "    counter_ - хранилище частот n-грам, в виде словаря счётчиков.\n",
    "    vocabulary_ - множество всех символов, учитываемых моделью\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, order=1, smoothing=1.0, recursive=0.001):\n",
    "        self.order = order\n",
    "        self.smoothing = smoothing\n",
    "        self.recursive = recursive\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        \"\"\" Оценка числа всех буквосочетаний по тексту\n",
    "        Параметры:\n",
    "        corpus - текстовая строка.\n",
    "        \"\"\"\n",
    "        self.counter_ = defaultdict(lambda: Counter())\n",
    "        self.vocabulary_ = set()\n",
    "        for i, token in enumerate(corpus[self.order:]):\n",
    "            context = corpus[i:(i+self.order)]\n",
    "            self.counter_[context][token] += 1\n",
    "            self.vocabulary_.add(token)\n",
    "        self.vocabulary_ = sorted(list(self.vocabulary_))\n",
    "        if self.recursive > 0 and self.order > 0:\n",
    "            self.child_ = LanguageNgramModel(self.order-1, self.smoothing, self.recursive)\n",
    "            self.child_.fit(corpus)\n",
    "\n",
    "    def get_counts(self, context):\n",
    "        \"\"\" Оценка частоты всех символов, которые могут следовать за контекстом\n",
    "        Параметры:\n",
    "        context - текстовая строка (учиываются только последние self.order символов)\n",
    "        Возвращает:\n",
    "        freq - вектор условных частот букв, в форме pandas.Series\n",
    "        \"\"\"\n",
    "        if self.order:\n",
    "            local = context[-self.order:]\n",
    "        else:\n",
    "            local = ''\n",
    "        freq_dict = self.counter_[local]\n",
    "        freq = pd.Series(index=self.vocabulary_)\n",
    "        for i, token in enumerate(self.vocabulary_):\n",
    "            freq[token] = freq_dict[token] + self.smoothing\n",
    "        if self.recursive > 0 and self.order > 0:\n",
    "            child_freq = self.child_.get_counts(context) * self.recursive\n",
    "            freq += child_freq\n",
    "        return freq\n",
    "\n",
    "    def predict_proba(self, context):\n",
    "        \"\"\" Сглаженная оценка вероятности всех символов, которые могут следовать за контекстом\n",
    "        Параметры:\n",
    "        context - текстовая строка (учиываются только последние self.order символов)\n",
    "        Возвращает:\n",
    "        freq - вектор условных вероятностей букв, в форме pandas.Series  \"\"\"\n",
    "        counts = self.get_counts(context)\n",
    "        return counts / counts.sum()\n",
    "\n",
    "    def single_log_proba(self, context, continuation):\n",
    "        \"\"\" Оценка логарифма вероятности конкретного продолжения данной фразы.\n",
    "        Параметры:\n",
    "        context - текстовая строка, известное начало фразы\n",
    "        continuation - текстовая строка, гипотетическое продолжение фразы\n",
    "        \"\"\"\n",
    "        result = 0.0\n",
    "        for token in continuation:\n",
    "            result += np.log(self.predict_proba(context)[token])\n",
    "            context += token\n",
    "        return result\n",
    "\n",
    "    def single_proba(self, context, continuation):\n",
    "        \"\"\" Оценка вероятности конкретного продолжения данной фразы.\n",
    "        Параметры:\n",
    "        context - текстовая строка, известное начало фразы\n",
    "        continuation - текстовая строка, гипотетическое продолжение фразы\n",
    "        \"\"\"\n",
    "        return np.exp(self.single_log_proba(context, continuation))\n",
    "\n",
    "\n",
    "class MissingLetterModel:\n",
    "    \"\"\" Модель запоминает и предсказывает, какие буквы обычно исключаются из сокращений\n",
    "    Параметры:\n",
    "    order - порядок, или n+1\n",
    "    smoothing_missed - число, прибавляемое к счётчику пропущенных символов\n",
    "    smoothing_total - число, прибавляемое к счётчику всех символов\n",
    "    \"\"\"\n",
    "    def __init__(self, order=0, smoothing_missed=0.3, smoothing_total=1.0):\n",
    "        self.order = order\n",
    "        self.smoothing_missed = smoothing_missed\n",
    "        self.smoothing_total = smoothing_total\n",
    "\n",
    "    def fit(self, sentence_pairs):\n",
    "        \"\"\" Оценка частоты сокращения символов на основе обучающих примеров\n",
    "        Параметры:\n",
    "        sentence_pairs - список пар (исходная фраза, сокращение)\n",
    "        В сокращении все пропущенные символы заменены на дефисы.\n",
    "        \"\"\"\n",
    "        self.missed_counter_ = defaultdict(lambda: Counter())\n",
    "        self.total_counter_ = defaultdict(lambda: Counter())\n",
    "        for (original, observed) in sentence_pairs:\n",
    "            for i, (original_letter, observed_letter) in enumerate(zip(original[self.order:], observed[self.order:])):\n",
    "                context = original[i:(i+self.order)]\n",
    "                if observed_letter == '-':\n",
    "                    self.missed_counter_[context][original_letter] += 1\n",
    "                self.total_counter_[context][original_letter] += 1\n",
    "\n",
    "    def predict_proba(self, context, last_letter):\n",
    "        \"\"\" Оценка вероятности того, что символ last_letter пропущен после символов context\"\"\"\n",
    "        if self.order:\n",
    "            local = context[-self.order:]\n",
    "        else:\n",
    "            local = ''\n",
    "        missed_freq = self.missed_counter_[local][last_letter] + self.smoothing_missed\n",
    "        total_freq = self.total_counter_[local][last_letter] + self.smoothing_total\n",
    "        return missed_freq / total_freq\n",
    "\n",
    "    def single_log_proba(self, context, continuation, actual=None):\n",
    "        \"\"\" Оценка логарифма вероятности того, после фразы context фраза continuation трансформируется в actual\n",
    "        Если actual не указана, предполагается, что continuation не изменяется.\n",
    "        \"\"\"\n",
    "        if not actual:\n",
    "            actual = continuation\n",
    "        result = 0.0\n",
    "        for orig_token, act_token in zip(continuation, actual):\n",
    "            pp = self.predict_proba(context, orig_token)\n",
    "            if act_token != '-':\n",
    "                pp = 1 - pp\n",
    "            result += np.log(pp)\n",
    "            context += orig_token\n",
    "        return result\n",
    "\n",
    "    def single_proba(self, context, continuation, actual=None):\n",
    "        \"\"\" Оценка вероятности того, после фразы context фраза continuation трансформируется в actual\n",
    "        Если actual не указана, предполагается, что continuation не изменяется.\n",
    "        \"\"\"\n",
    "        return np.exp(self.single_log_proba(context, continuation, actual))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_options(prefix_proba, prefix, suffix, lang_model, missed_model, optimism=0.5, cache=None):\n",
    "    \"\"\" Генерация вариантов расшифровки аббревиатуры (вспомогательная функция)\n",
    "    Параметры:\n",
    "    prefix_proba - правдоподобие расшифрованной части аббревиатуры\n",
    "    prefix - расшифрованная часть аббревиатуры\n",
    "    suffix - не расшифрованная часть аббревиатуры\n",
    "    lang_model - модель языка\n",
    "    missed_model - модель вероятности сокращений\n",
    "    optimism - коэффициент, с которым учитывается не объясненный конец слова\n",
    "    cache - хранилище оценок качества концов слова\n",
    "    Возвращает: список опций в форме (оценка качества, расшифрованная часть,\n",
    "        не расшифрованная часть, новая буква, оценка качества не расшифрованной части)\n",
    "    \"\"\"\n",
    "    options = []\n",
    "    for letter in lang_model.vocabulary_ + ['']:\n",
    "        if letter:  # тут мы считаем, что буква была пропущена\n",
    "            next_letter = letter\n",
    "            new_suffix = suffix\n",
    "            new_prefix = prefix + next_letter\n",
    "            proba_missing_state = - np.log(missed_model.predict_proba(prefix, letter))\n",
    "        else:  # тут мы считаем, что пропущенной буквы не было\n",
    "            next_letter = suffix[0]\n",
    "            new_suffix = suffix[1:]\n",
    "            new_prefix = prefix + next_letter\n",
    "            proba_missing_state = - np.log((1 - missed_model.predict_proba(prefix, next_letter)))\n",
    "        proba_next_letter = - np.log(lang_model.single_proba(prefix, next_letter))\n",
    "        if cache:\n",
    "            proba_suffix = cache[len(new_suffix)] * optimism\n",
    "        else:\n",
    "            proba_suffix = - np.log(lang_model.single_proba(new_prefix, new_suffix)) * optimism\n",
    "        proba = prefix_proba + proba_next_letter + proba_missing_state + proba_suffix\n",
    "        options.append((proba, new_prefix, new_suffix, letter, proba_suffix))\n",
    "    return options\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# считываем текст\n",
    "# with open('Pofigizm_book.txt', encoding = 'utf-8') as f:\n",
    "#     text = f.read()\n",
    "\n",
    "# оставляем только буквы и пробелы в тексте\n",
    "text2 = re.sub(r'[^а-я ]+', '', text.lower().replace('\\n', ' '))\n",
    "all_letters = ''.join(list(sorted(list(set(text2)))))\n",
    "print(repr(all_letters)) # ' абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "# готовим обучающую выборку для модели опечаток:\n",
    "missing_set =  (\n",
    "    [(all_letters, '-' * len(all_letters))] * 3 # тут считаем все буквы пропущенными\n",
    "    + [(all_letters, all_letters)] * 10 # тут считаем все буквы НЕ пропущенными\n",
    "    + [('аоуыэеёиюя', '----------')] * 30 # тут считаем пропущенными только гласные\n",
    ")\n",
    "# обучаем обе модели\n",
    "big_lang_m = LanguageNgramModel(order=6, smoothing=0.005, recursive=0.01)\n",
    "big_lang_m.fit(text2)\n",
    "big_err_m = MissingLetterModel(order=0, smoothing_missed=0.1)\n",
    "big_err_m.fit(missing_set)\n",
    "\n",
    "for i in range(5):\n",
    "    tmp = LanguageNgramModel(i, 0.001, 0.01)\n",
    "    tmp.fit(text2[0:-5000])\n",
    "    print(i, tmp.single_log_proba(' ', text2[-5000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca7a24c42f07a440",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T15:18:38.706166Z",
     "start_time": "2024-03-26T15:15:23.840151Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'пжлст': 22.486915722981173}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def noisy_channel(word, lang_model, missed_model, freedom=3.0, max_attempts=2000, optimism=0.92, verbose=False):\n",
    "    \"\"\" Подбор фраз, аббревиатурой которых может быть word\n",
    "    Параметры:\n",
    "    word - аббревиатура\n",
    "    lang_model - модель языка\n",
    "    missed_model - модель вероятности сокращений\n",
    "    freedom - возможный зазор по оценке логарифма правдоподобия кандидатов\n",
    "    max_attempts - число итераций\n",
    "    optimism - коэффициент, с которым учитывается не объясненный конец слова\n",
    "    verbose - печатать ли наилучших текущих кандидатов в ходе исполнения функции\n",
    "    Возвращает: словарик с ключами - расшифровками\n",
    "        и значениями - минус логарифмом правдоподобия расшифровок.\n",
    "        Чем меньше значение, тем правдоподобнее расшифровка.\n",
    "    \"\"\"\n",
    "    query = word + ' '\n",
    "    prefix = ' '\n",
    "    prefix_proba = 0.0\n",
    "    suffix = query\n",
    "    full_origin_logprob = -lang_model.single_log_proba(prefix, query)\n",
    "    no_missing_logprob = -missed_model.single_log_proba(prefix, query)\n",
    "    best_logprob = full_origin_logprob + no_missing_logprob\n",
    "    # добавляем в кучу пустое начало\n",
    "    heap = [(best_logprob * optimism, prefix, suffix, '', best_logprob * optimism)]\n",
    "    # добавляем в кандидаты расшифровку по умолчанию - без пропущенных букв\n",
    "    candidates = [(best_logprob, prefix + query, '', None, 0.0)]\n",
    "    if verbose:\n",
    "        print('baseline score is', best_logprob)\n",
    "    # готовим хранилище вероятностей конфов слов\n",
    "    cache = {}\n",
    "    for i in range(len(query)+1):\n",
    "        future_suffix = query[:i]\n",
    "        cache[len(future_suffix)] = -lang_model.single_log_proba('', future_suffix) # rough approximation\n",
    "        cache[len(future_suffix)] += -missed_model.single_log_proba('', future_suffix) # at least add missingness\n",
    "\n",
    "    for i in range(max_attempts):\n",
    "        if not heap:\n",
    "            break\n",
    "        next_best = heappop(heap)\n",
    "        if verbose:\n",
    "            print(next_best)\n",
    "        if next_best[2] == '':  # слово расшифровано до конца\n",
    "            # если оно достаточно хорошее, добавим его в кандидаты\n",
    "            if next_best[0] <= best_logprob + freedom:\n",
    "                candidates.append(next_best)\n",
    "                # обновим наилучшую оценку правдоподобия\n",
    "                if next_best[0] < best_logprob:\n",
    "                    best_logprob = next_best[0]\n",
    "        else: # it is not a leaf - generate more options\n",
    "            prefix_proba = next_best[0] - next_best[4] # all proba estimate minus suffix\n",
    "            prefix = next_best[1]\n",
    "            suffix = next_best[2]\n",
    "            new_options = generate_options(prefix_proba, prefix, suffix, lang_model, missed_model, optimism, cache)\n",
    "            # add only the solution potentioally no worse than the best + freedom\n",
    "            for new_option in new_options:\n",
    "                if new_option[0] < best_logprob + freedom:\n",
    "                    heappush(heap, new_option)\n",
    "    if verbose:\n",
    "        print('heap size is', len(heap), 'after', i, 'iterations')\n",
    "    result = {}\n",
    "    for candidate in candidates:\n",
    "        if candidate[0] <= best_logprob + freedom:\n",
    "            result[candidate[1][1:-1]] = candidate[0]\n",
    "    return result\n",
    "\n",
    "noisy_channel('пжлст', big_lang_m, big_err_m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
